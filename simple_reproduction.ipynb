{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple reproduction - GEO-AI Challenge for Cropland Mapping by ITU\n",
    "_Antoine Saget_\n",
    "\n",
    "In this notebook, the results are reproduced with a more streamlined code that might be easier to integrate in a production pipeline than the original code as the original code was written only for the purpose of the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on Google Colab, please run this cell first to clone the GitHub repository \n",
    "!git clone https://github.com/antoinesaget/geo_ai_cropland_mapping_submission.git\n",
    "%cd geo_ai_cropland_mapping_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PLEASE SET THIS VARIABLE TO YOUR PROJECT NAME IN GEE #####\n",
    "PROJECT_NAME = 'ee-antoinesaget'\n",
    "\n",
    "# Uncomment if downloading data from scratch\n",
    "# ee.Authenticate()\n",
    "# ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and seeds initializations\n",
    "import random\n",
    "import ee\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 2023\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Dataframe column names\n",
    "TS_ID = 'TS_ID'\n",
    "ID = 'ID'\n",
    "TARGET = 'Target'\n",
    "LAT = 'Lat'\n",
    "LON = 'Lon'\n",
    "TIMESTAMP = 'Timestamp'\n",
    "COUNTRY = 'Country'\n",
    "IS_TRAIN = 'IsTrain'\n",
    "\n",
    "B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12 = 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'\n",
    "SCL = 'SCL'\n",
    "NDVI = 'NDVI'\n",
    "\n",
    "ALL_BANDS = [B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12, SCL, NDVI]\n",
    "BEST_BANDS = [B3, B4, B8, LON, LAT, NDVI]\n",
    "\n",
    "SUDAN = 'Sudan'\n",
    "AFGHANISTAN = 'Afghanistan'\n",
    "IRAN = 'Iran'\n",
    "\n",
    "COUNTRY_NAME = 'Country Name'\n",
    "START_DATE = 'Start Date'\n",
    "END_DATE = 'End Date'\n",
    "BOUNDS = 'Bounds'\n",
    "\n",
    "# GEE Collection name\n",
    "COLLECTION_NAME = 'COPERNICUS/S2_SR_HARMONIZED'\n",
    "\n",
    "CACHE_FOLDER = 'data/'\n",
    "\n",
    "# Country bounds and timeranges\n",
    "country_settings = {\n",
    "   SUDAN: {\n",
    "        COUNTRY_NAME: SUDAN,\n",
    "        START_DATE: '2019-07-01',\n",
    "        END_DATE: '2020-06-30',\n",
    "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
    "    },\n",
    "    AFGHANISTAN: {\n",
    "        COUNTRY_NAME: AFGHANISTAN,\n",
    "        START_DATE: '2022-04-01',\n",
    "        END_DATE: '2022-04-30',\n",
    "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
    "    },\n",
    "    IRAN: {\n",
    "        COUNTRY_NAME: IRAN,\n",
    "        START_DATE: '2019-07-01',\n",
    "        END_DATE: '2020-06-30',\n",
    "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
    "    }\n",
    "}\n",
    "\n",
    "BASENAME = f'projects/{PROJECT_NAME}/assets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "def filter_by_bounds_mask(points, bounds):\n",
    "    \"\"\"\n",
    "    Returns a boolean mask of the points that are inside the bounds\n",
    "    \"\"\"\n",
    "    min_, max_ = bounds\n",
    "    min_lat, min_lon = min_\n",
    "    max_lat, max_lon = max_\n",
    "\n",
    "    return (\n",
    "        (points[LAT] >= min_lat) &\n",
    "        (points[LAT] <= max_lat) &\n",
    "        (points[LON] >= min_lon) &\n",
    "        (points[LON] <= max_lon)\n",
    "    )\n",
    "\n",
    "def filter_by_country(df, country):\n",
    "    if country is None:\n",
    "        return np.ones(df.shape[0]).astype('bool')\n",
    "    return df[COUNTRY] == country[COUNTRY_NAME]\n",
    "\n",
    "def filter_by_dates(points, start_date, end_date):\n",
    "    return points.loc[\n",
    "        (points[TIMESTAMP] >= start_date) &\n",
    "        (points[TIMESTAMP] <= end_date)\n",
    "    ]\n",
    "\n",
    "def fc_to_dict(fc):\n",
    "    prop_names = fc.first().propertyNames()\n",
    "    prop_lists = fc.reduceColumns(\n",
    "        reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
    "        selectors=prop_names).get('list')\n",
    "\n",
    "    return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
    "\n",
    "def prepare_df(df, stats_dict):\n",
    "    bands = ALL_BANDS\n",
    "    stats_df = pd.DataFrame(stats_dict)\n",
    "\n",
    "    def add_date_info(df):\n",
    "        df[TIMESTAMP] = pd.to_datetime(df['millis'], unit='ms')\n",
    "        return df\n",
    "\n",
    "    stats_df = add_date_info(stats_df)\n",
    "    stats_df = stats_df.drop(columns=['millis', 'system:index'])\n",
    "    \n",
    "    stats_df['TS_ID'] = stats_df['TS_ID'].astype('uint')\n",
    "    ts_ids = np.unique(stats_df['TS_ID'])\n",
    "\n",
    "    assert len(ts_ids) == len(df)\n",
    "\n",
    "    for ts_id, id in zip(np.unique(stats_df['TS_ID']), df.index.values):\n",
    "        stats_df.loc[stats_df['TS_ID'] == ts_id, ID] = id\n",
    "    stats_df = stats_df.drop(columns=['TS_ID'])\n",
    "        \n",
    "    stats_df.sort_values(by=[ID, TIMESTAMP], inplace=True)\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def start_download_task(df, country, start_date, end_date, bands, filename, scale=10):\n",
    "    datapoints = df[filter_by_country(df, country)]\n",
    "    points_fc = fc_from_points(datapoints)\n",
    "    collection = (ee\n",
    "        .ImageCollection(self.collection_name)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filterBounds(points_fc)\n",
    "    )\n",
    "\n",
    "    if 'NDVI' in bands:\n",
    "        collection = collection.map(addNDVI)\n",
    "\n",
    "    def image_reducer(image):\n",
    "        def feature_reducer(feature):\n",
    "            coordinates = feature.geometry().coordinates()\n",
    "            di = {\n",
    "                'millis': image.date().millis(),\n",
    "                TS_ID: feature.id(),\n",
    "                LON: coordinates.get(0),\n",
    "                LAT: coordinates.get(1)\n",
    "            }\n",
    "            \n",
    "            for band in bands:\n",
    "                band_filtered = ee.List([feature.get(band), -9999]).reduce(ee.Reducer.firstNonNull())\n",
    "                di[band] = band_filtered\n",
    "\n",
    "            return feature.set(di)\n",
    "\n",
    "        return image.select(bands).reduceRegions(\n",
    "            collection=points_fc,\n",
    "            reducer=ee.Reducer.first(),\n",
    "            scale=scale,\n",
    "        ).map(feature_reducer)\n",
    "\n",
    "    stats_fc = (ee.FeatureCollection(collection\n",
    "        .map(image_reducer)\n",
    "        .flatten()\n",
    "        .filter(ee.Filter.neq(bands[0], -9999))\n",
    "        .distinct(['system:index', 'millis'])\n",
    "    ))\n",
    "\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=stats_fc,\n",
    "        description='stats_fc export',\n",
    "        assetId=self.basename + filename,\n",
    "    )\n",
    "    task.start()\n",
    "    return task\n",
    "\n",
    "def interpolate_nans_3D(X):\n",
    "    n_objects, n_timesteps, n_bands = X.shape\n",
    "    isnan = np.isnan(X[:, :, 0])\n",
    "    for i, x in enumerate(X):\n",
    "        valid_timesteps = np.arange(n_timesteps)[~isnan[i]]\n",
    "        valid_values = X[i, valid_timesteps]\n",
    "\n",
    "        # Perform linear interpolation and extrapolation\n",
    "        for j in range(n_bands):\n",
    "            X[i, :, j] = np.interp(\n",
    "                np.arange(n_timesteps), \n",
    "                valid_timesteps,\n",
    "                valid_values[:, j]\n",
    "            )\n",
    "    return X\n",
    "\n",
    "def interpolate_ts(df, bands, sampling_rate=5, start_date=None, end_date=None):\n",
    "    df = df.sort_values(by=[ID, TIMESTAMP])\n",
    "\n",
    "    # bands_train is a list of timeseries of shape (n_timesteps, n_bands)\n",
    "    # but each timeserie can have a different number of timestamps so we will interpolate \n",
    "    # the missing values between start and end date\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = pd.to_datetime(df[TIMESTAMP].min())\n",
    "    if end_date is None:\n",
    "        end_date = pd.to_datetime(df[TIMESTAMP].max())\n",
    "    date_range = pd.date_range(start_date, end_date, freq='D', normalize=True)\n",
    "\n",
    "    df[TIMESTAMP] = pd.to_datetime(df[TIMESTAMP]).dt.normalize()\n",
    "    df = df.drop_duplicates(subset=[ID, TIMESTAMP])\n",
    "\n",
    "    groups = df.groupby(ID)\n",
    "    XX = groups.apply(\n",
    "        lambda x: x.set_index(TIMESTAMP)[bands].reindex(date_range).values\n",
    "    )\n",
    "    IDs = groups[ID].first().values\n",
    "\n",
    "    XX = np.stack(XX.values).astype('float32')\n",
    "    XX = interpolate_nans_3D(XX)\n",
    "    XX = XX[:, ::sampling_rate, :]\n",
    "\n",
    "    return XX, IDs\n",
    "\n",
    "def save_submission(preds, ids, filename):\n",
    "    df = pd.DataFrame({'ID': ids, 'Target': preds})\n",
    "    time = pd.Timestamp.now().strftime('%m_%d_%Hh_%Mm_%Ss')\n",
    "    folder = 'submissions'\n",
    "    \n",
    "    filename = f'{folder}/{time}_{filename}.csv'\n",
    "    \n",
    "    df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "\n",
    "# A single pandas Dataframe for test and train with Target at null for test\n",
    "df = pd.concat([train, test])\n",
    "\n",
    "# Add a convenient column to distinguish between train and test\n",
    "df[IS_TRAIN] = df[TARGET].notnull()\n",
    "\n",
    "# Add a column with the country name of each point\n",
    "for country in country_settings.values():\n",
    "    mask = filter_by_bounds_mask(df, country[BOUNDS])\n",
    "    df.loc[mask, COUNTRY] = country[COUNTRY_NAME]\n",
    "\n",
    "df = df.set_index(ID)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optical data if available locally, otherwise download it from GEE\n",
    "optical_df = []\n",
    "tasks = []\n",
    "\n",
    "for country in country_settings.values():\n",
    "    # We load 1 year before and 1 year after the start and end date \n",
    "    start_date = (pd.to_datetime(country[START_DATE]) - pd.DateOffset(days=365)).strftime('%Y-%m-%d')\n",
    "    end_date = (pd.to_datetime(country[END_DATE]) + pd.DateOffset(days=365)).strftime('%Y-%m-%d')\n",
    "\n",
    "    filename = (f'{COLLECTION_NAME.replace(\"/\", \"_\")}_'\n",
    "        + f'{country[COUNTRY_NAME]}_'\n",
    "        + f'{str(start_date)}_{str(end_date)}'\n",
    "    )\n",
    "\n",
    "    # 1. Check if the file exists locally\n",
    "    try:\n",
    "        optical_df.append(pd.read_csv(CACHE_FOLDER + filename + '.csv'))\n",
    "        continue\n",
    "    except FileNotFoundError:\n",
    "        print(f'File for {country[COUNTRY_NAME]} does not exist. We will download it from GEE first.')\n",
    "\n",
    "    # 2. If not, check if it exists in GEE\n",
    "    try:\n",
    "        stats_fcc = ee.FeatureCollection(BASENAME + filename)\n",
    "        stats_dict = fc_to_dict(stats_fcc).getInfo()\n",
    "\n",
    "        df_country = prepare_df(df[filter_by_country(df, country)], stats_dict)\n",
    "        df_country.to_csv(CACHE_FOLDER + filename + '.csv', index=False)\n",
    "        optical_df.append(df_country)\n",
    "        continue\n",
    "    except ee.EEException as e:\n",
    "        print(f'File for {country[COUNTRY_NAME]} does not exist in GEE. We will start a task to download it.')\n",
    "\n",
    "    # 3. If not, start a task to download it in GEE\n",
    "    task = start_download_task(df, country, start_date, end_date, ALL_BANDS, filename)\n",
    "    tasks.append([task, country[COUNTRY_NAME]])\n",
    "\n",
    "# If there are tasks running, wait for them to finish\n",
    "if len(tasks) > 0:\n",
    "    print('Downloading from ee...')\n",
    "    print('This may take a while (up to 40 minutes).')\n",
    "    print('This requires a Google Earth Engine account and at least 30MB of available storage space.')\n",
    "    print('Please note that this download step is only required once.')\n",
    "\n",
    "    wait_for_tasks(tasks)\n",
    "\n",
    "    # Once the tasks are finished, load the data from GEE and save it locally\n",
    "    for country in countries_settings.values():\n",
    "        start_date = (pd.to_datetime(country[START_DATE]) - pd.DateOffset(days=365)).strftime('%Y-%m-%d')\n",
    "        end_date = (pd.to_datetime(country[END_DATE]) + pd.DateOffset(days=365)).strftime('%Y-%m-%d')\n",
    "\n",
    "        filename = (f'{COLLECTION_NAME.replace(\"/\", \"_\")}_'\n",
    "            + f'{country[COUNTRY_NAME]}_'\n",
    "            + f'{str(start_date)}_{str(end_date)}'\n",
    "        )\n",
    "        \n",
    "        stats_fcc = ee.FeatureCollection(BASENAME + filename)\n",
    "        stats_dict = fc_to_dict(stats_fcc).getInfo()\n",
    "\n",
    "        df_country = prepare_df(df[filter_by_country(df, country)], stats_dict)\n",
    "        df_country.to_csv(CACHE_FOLDER + filename + '.csv', index=False)\n",
    "        optical_df.append(df_country)\n",
    "\n",
    "# Concatenate all country dataframes into a single one\n",
    "optical_df = pd.concat(optical_df)\n",
    "\n",
    "optical_df = optical_df.set_index(ID)\n",
    "display(optical_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have : \n",
    "- `df` : contains train and test data from 'Train.csv' and 'Test.csv'\n",
    "- `optical_df` : conatains train and test optical timeseries data from GEE Sentinel-2\n",
    "\n",
    "In `optical_df` each row is a timestep and once grouped by same `ID` they form the timeseries for each data point.\n",
    "\n",
    "At this point, each time serie might be of different length. \n",
    "In the next step we will preprocess them to have a fixed length and aligned timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [B2, B3, B4, B8, LON, LAT, NDVI, SCL]\n",
    "\n",
    "# For more details on how the timeranges were chosen, \n",
    "# please refer to section 2. of GEO_AI_Cropland_extent_antoine_saget.ipynb\n",
    "country_settings_optimal = {\n",
    "    SUDAN: {\n",
    "        COUNTRY_NAME: SUDAN,\n",
    "        START_DATE: '2019-05-29',\n",
    "        END_DATE: '2021-03-31',\n",
    "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
    "    },\n",
    "    AFGHANISTAN: {\n",
    "        COUNTRY_NAME: AFGHANISTAN,\n",
    "        START_DATE: '2021-06-27',\n",
    "        END_DATE: '2023-04-30',\n",
    "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
    "    },\n",
    "    IRAN: {\n",
    "        COUNTRY_NAME: IRAN,\n",
    "        START_DATE: '2018-08-28',\n",
    "        END_DATE: '2020-06-30',\n",
    "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optical_df_filtered = pd.DataFrame(columns=[TIMESTAMP] + bands)\n",
    "\n",
    "# Filtering out optical data outside of the optimal timeranges\n",
    "for country in country_settings_optimal.values():\n",
    "    country_mask = filter_by_country(df, country)\n",
    "    country_mask = country_mask.reindex(optical_df.index, fill_value=False)\n",
    "\n",
    "    optical = optical_df[country_mask]\n",
    "    optical = filter_by_dates(optical, country[START_DATE], country[END_DATE])\n",
    "    optical = optical[[TIMESTAMP] + bands]\n",
    "\n",
    "    optical_df_filtered = pd.concat([optical_df_filtered, optical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the SCL int column into multiple boolean columns\n",
    "for scl in optical_df_filtered[SCL].unique():\n",
    "    SCL_COL = f'{SCL}_{scl}'\n",
    "    optical_df_filtered[SCL_COL] = optical_df_filtered[SCL] == scl\n",
    "    optical_df_filtered[SCL_COL] = optical_df_filtered[SCL_COL].astype('uint8')\n",
    "    bands.append(SCL_COL)\n",
    "bands.remove(SCL)\n",
    "optical_df_filtered = optical_df_filtered.drop(columns=[SCL])\n",
    "optical_df_filtered = optical_df_filtered.reset_index(names=ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's interpolate the optical data to have a fixed and aligned number of timesteps\n",
    "dfs_preprocessed = {}\n",
    "for country in country_settings_optimal.values():\n",
    "    country_mask = df[COUNTRY] == country[COUNTRY_NAME]\n",
    "    ids = df.loc[country_mask].index\n",
    "    \n",
    "    optical_filtered = optical_df_filtered.loc[optical_df_filtered[ID].isin(ids)]\n",
    "    X, IDs = interpolate_ts(optical_filtered, bands, start_date=optical_df_filtered[TIMESTAMP].min(), end_date=optical_df_filtered[TIMESTAMP].max())\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X = pd.DataFrame(X, index=IDs)\n",
    "\n",
    "    X_train = X.loc[X.index.isin(df.loc[df[IS_TRAIN] & country_mask].index)].sort_index()\n",
    "    Y_train = df.loc[df[IS_TRAIN] & country_mask, TARGET].sort_index().astype('uint8')\n",
    "    X_test = X.loc[X.index.isin(df.loc[~df[IS_TRAIN] & country_mask].index)].sort_index()\n",
    "\n",
    "    dfs_preprocessed[country[COUNTRY_NAME]] = {\n",
    "        'X_train': X_train,\n",
    "        'Y_train': Y_train,\n",
    "        'X_test': X_test,\n",
    "    }\n",
    "    \n",
    "print('Train data for SUDAN :')\n",
    "display(dfs_preprocessed[SUDAN]['X_train'])\n",
    "print('Train labels for SUDAN :')\n",
    "display(dfs_preprocessed[SUDAN]['Y_train'])\n",
    "print('Test data for SUDAN :')\n",
    "display(dfs_preprocessed[SUDAN]['X_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have fixed length timeseries with aligned timesteps separated in three country subsets.\n",
    "For each country subset we have :\n",
    "- `X_train` : the flatten timeserie train data, each row is a timeserie\n",
    "- `Y_train` : the label corresponding to each timeserie\n",
    "- `X_test` : the flatten timeserie test data, each row is a timeserie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the model\n",
    "\n",
    "Given the relatively minor preprocessing, we use a RandomForest as it will be robust to problematic timesteps such as cloudy ones.\n",
    "Indeed, the RandomForest will be able to ignore them and only use features that are relevant.\n",
    "For good results with other methods, further preprocessing such as cloud filtering might be necessary.\n",
    "\n",
    "For better generalization, we limit the depth of the trees in the random forest.\n",
    "Also, we train one random forest per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDS = []\n",
    "IDS = []\n",
    "\n",
    "for country in country_settings_optimal.values():\n",
    "    X_train = dfs_preprocessed[country[COUNTRY_NAME]]['X_train']\n",
    "    Y_train = dfs_preprocessed[country[COUNTRY_NAME]]['Y_train']\n",
    "    X_test = dfs_preprocessed[country[COUNTRY_NAME]]['X_test']\n",
    "\n",
    "    model = RandomForestClassifier(random_state = SEED, n_estimators=100, max_depth=10)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    PREDS += list(predictions)\n",
    "    IDS += list(X_test.index.values)\n",
    "\n",
    "PREDS = pd.Series(index=IDS, dtype='uint8', name='Pred', data=PREDS)\n",
    "PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to subsmission to make sure they are the same\n",
    "original = pd.read_csv('submissions/original_challenge_submission.csv', index_col='ID', usecols=['ID', TARGET])\n",
    "original[TARGET] = original[TARGET].astype('uint8')\n",
    "\n",
    "diff = original.loc[IDS, TARGET] - PREDS\n",
    "diff = diff[diff != 0]\n",
    "print(f'Number of predictions different from original submission : {len(diff)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(PREDS, IDS, 'reproduction_of_original_submission')\n",
    "# Please note that a diff with the original submission and this one will not be 0.\n",
    "# By mistake, the original submission also included predictions of the training set.\n",
    "# This mean that the original submission is 3000 rows while this one is 1500 rows (only test set)\n",
    "# But the predictions on the test set are the same (as shown in the above cell)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
