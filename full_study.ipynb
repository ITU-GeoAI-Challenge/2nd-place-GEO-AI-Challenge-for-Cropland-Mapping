{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncvg53yd-bP5"
      },
      "source": [
        "# GEO-AI Challenge for Cropland Mapping by ITU\n",
        "_Antoine Saget_\n",
        "\n",
        "In this notebook, we present our solution for the Zindi GEO-AI Challenge for Cropland Mapping by ITU to achieve a 0.943 accuracy on the private leaderboard.  \n",
        "Our solution is based on a simple Random Forest classifier trained on Sentinel-2 timeserie data.  \n",
        "Our contribution lies mostly in retrieving the Sentinel-2 timeseries data from Google Earth Engine.  \n",
        "The preprocessing is kept to a minimum as the Random Forest classifier is robust to unclean data.  \n",
        "\n",
        "We also provide a second notebook (`simple_reproduction.ipynb`) with simpler code that reproduce the same results.  \n",
        "This other notebook should be easier to integrate in your own workflow as it doesn't rely on any additional file and classes.\n",
        "\n",
        "- Section 1.-4. are dependant and cells must be executed in order.  \n",
        "- Section 5. is independant and can be executed without running the previous sections.  \n",
        "- You can skip to 5. to only reproduce the private leaderboard solutions or start from 1. to get a better understanding of the data download and prepprocessing steps.  \n",
        "- Cells that take a long time to execute are marked with a warning and approximate execution time.  \n",
        "\n",
        "The notebook is divided in 6 sections as follows:\n",
        "1. Downloading the data from GEE\n",
        "2. Data preprocessing to obtain aligned timeseries of same lengths\n",
        "3. Study of different time spans (length) and periods (start date)\n",
        "4. Study of different Sentinel-2 radiometric bands\n",
        "5. Reproduction of the submitted solution\n",
        "6. Discussion on strengths, weaknesses and possible improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_wLzoiFBuCr"
      },
      "outputs": [],
      "source": [
        "# Imports and seeds initializations\n",
        "import ee\n",
        "import folium\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from src.utils import filter_by_country, interpolate_ts, base_folium\n",
        "from src.dataset import Dataset, Dataset_training_ready\n",
        "from src.constants import SUDAN, AFGHANISTAN, IRAN, COUNTRY_NAME, START_DATE, END_DATE, BOUNDS, TARGET, B2, B3, B4, B8, LON, LAT, NDVI, SCL, PROJECT_NAME, ID, IS_TRAIN, COUNTRY, ALL_BANDS, C_CLASS_0, C_CLASS_1, C_TEST, MK_BASE_RADIUS\n",
        "from src.model import Model, rf_builder, rf_builder_shallow, rf_builder_big\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 2023\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Country bounds and timeranges\n",
        "country_settings = {\n",
        "   SUDAN: {\n",
        "        COUNTRY_NAME: SUDAN,\n",
        "        START_DATE: '2019-07-01',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
        "    },\n",
        "    AFGHANISTAN: {\n",
        "        COUNTRY_NAME: AFGHANISTAN,\n",
        "        START_DATE: '2022-04-01',\n",
        "        END_DATE: '2022-04-30',\n",
        "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
        "    },\n",
        "    IRAN: {\n",
        "        COUNTRY_NAME: IRAN,\n",
        "        START_DATE: '2019-07-01',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUMI_y00ANrX"
      },
      "source": [
        "# 1. Downloading the data from GEE\n",
        "\n",
        "In this part, timeserie data from Sentinel-2 is downloaded from GEE.\n",
        "\n",
        "When cloning the git repo, the data is already pre-dowloaded in the data folder.\n",
        "If you want to download it from scratch, remove files in the data folder and run the following cells.\n",
        "Redownloading the data from scratch can take up to 1h30~.\n",
        "\n",
        "Please note that both options output the exact same data as of 06/10/2023 as the pre-dowloaded data is just a collection of .csv files saved from the data obtained with GEE. However, with possible future changes to the GEE Sentinel-2 collection, the pre-downloaded data might get outdated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVaj03wBBYlj",
        "outputId": "49211378-6813-46d9-bb6d-94ee9cc49af3"
      },
      "outputs": [],
      "source": [
        "# This cell is only necessary if you want to redownload the data from scratch\n",
        "# If data is already pre-downloaded in data/, you can skip it\n",
        "\n",
        "# Authenticate and initialize Earth Engine\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "##### PLEASE SET THIS VARIABLE TO YOUR PROJECT NAME IN GEE #####\n",
        "PROJECT_NAME = \"ee-antoinesaget\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8TpdxH4C4ck"
      },
      "outputs": [],
      "source": [
        "ds = Dataset.from_files('Train.csv', 'Test.csv', 'Full dataset', country_settings, debug_level=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the dataset on an interactive folium map\n",
        "m = base_folium(ds)\n",
        "\n",
        "fg_train_0 = folium.FeatureGroup(name='Train Class 0')\n",
        "fg_train_1 = folium.FeatureGroup(name='Train Class 1')\n",
        "fg_test = folium.FeatureGroup(name='Test')\n",
        "\n",
        "for lat, lon, target in zip(ds.train['Lat'], ds.train['Lon'], ds.train['Target']):\n",
        "    if target == 0:\n",
        "        c = C_CLASS_0\n",
        "        fg = fg_train_0\n",
        "    elif target == 1:\n",
        "        c = C_CLASS_1\n",
        "        fg = fg_train_1\n",
        "    \n",
        "    marker = folium.CircleMarker(\n",
        "        location=[lat, lon], radius=MK_BASE_RADIUS, color=c, fill_color=c)\n",
        "    fg.add_child(marker)\n",
        "\n",
        "for lat, lon in zip(ds.test['Lat'], ds.test['Lon']):\n",
        "    marker = folium.CircleMarker(\n",
        "        location=[lat, lon], radius=MK_BASE_RADIUS, color=C_TEST, fill_color=C_TEST)  \n",
        "    fg_test.add_child(marker)\n",
        "\n",
        "m.add_child(fg_train_0)\n",
        "m.add_child(fg_train_1)\n",
        "m.add_child(fg_test)\n",
        "\n",
        "m.add_child(folium.LayerControl())\n",
        "\n",
        "display(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds.load_all_optical_data(PROJECT_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The timeserie data downloaded from GEE is as follow:\n",
        "\n",
        "For each datapoint in the training and test sets, we dowload all timesteps from 1 year before and 1 year after the challenges given ranges with the following parameters:\n",
        "- No cloud filtering, cloudy timesteps are kept\n",
        "- Filter out of nan/null values\n",
        "- All Sentinel-2 bands + SCL + NDVI\n",
        "- 10m resolution\n",
        "\n",
        "_Please see `downloader.py/_start_download_task()` function or `simple_reproduction.ipynb/start_download_task()` function for more details on the GEE task._\n",
        "\n",
        "This will result in multivariate timeseries with a different number of timesteps and different starting and ending dates for each datapoint.\n",
        "We illustrate this with histograms of number of timesteps below :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for country, ax in zip(ds.countries, axs):\n",
        "    mask = filter_by_country(ds._df, country)\n",
        "    ids = ds._df[mask][ID]\n",
        "    optical_country = ds._optical_df[ds._optical_df[ID].isin(ids)]\n",
        "    optical_country.groupby(ID).count()[NDVI].hist(bins=100, ax=ax)\n",
        "\n",
        "    ax.set_title(f'Histogram of number of timesteps for {country.name}')\n",
        "    ax.set_xlabel('Number of timesteps')\n",
        "    ax.set_ylabel('Number of datapoints')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the number of timesteps varies a lot between datapoints : from 100 up to 700+ timesteps.  \n",
        "This variation occurs even within the same country.\n",
        "\n",
        "This is due to the fact that Sentinel-2 revisit time is not always 5 days due to tile overlaping and other factors.\n",
        "\n",
        "Timeseries with different number of timesteps cannot be trivially fed to a supervised model like a Random Forest as the number of features will vary between datapoints.  \n",
        "In the next section we show how we deal with this issue with a simple interpolation and reindexing method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our solution, the only data preprocessing we add after gathering the timeseries data from GEE is a simple interpolation and reindexing method to deal with the varying number of timesteps and alignement issues.\n",
        "\n",
        "Further preprocessing such as cloud filtering, standaridzation or normalization might improve the results, especially for models not as robust to outliers as Random Forests or for models sensitive to scale such as Neural Networks.\n",
        "\n",
        "However, in this notebook, no further preprocessing is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reindexing and interpolation is done with the `utils.py/interpolate_ts()` function. Here is a simplified version of the function:\n",
        "\n",
        "```python\n",
        "# Please not this is a simplified version of the function that might not work as is\n",
        "def interpolate_ts(df, start_date, end_date, sampling_rate):\n",
        "    # A range of date from start_date to end_date with one timestep every day\n",
        "    date_range = pd.date_range(start_date, end_date, freq='D', normalize=True)\n",
        "\n",
        "    # Currently, each row is a timestep, a timeserie is a group of rows with the same ID\n",
        "    groups = df.groupby(ID)\n",
        "\n",
        "    # For each timeserie, we reindex the rows with the date_range\n",
        "    # This will create rows with nan values for missing timesteps\n",
        "    X = groups.apply(\n",
        "        lambda x: x.set_index(TIMESTAMP)[bands].reindex(date_range).values\n",
        "    )\n",
        "\n",
        "    # Convert the timeseries to a 3D numpy array of shape (n_objects, n_timesteps, n_bands)\n",
        "    X = np.stack(X.values).astype('float32')\n",
        "    n_objects, n_timesteps, n_bands = X.shape\n",
        "    \n",
        "    # A mask of all nan timesteps\n",
        "    isnan = np.isnan(X[:, :, 0])\n",
        "\n",
        "    # For each timeserie\n",
        "    for i, x in enumerate(X):\n",
        "        valid_timesteps = np.arange(n_timesteps)[~isnan[i]]\n",
        "        valid_values = X[i, valid_timesteps]\n",
        "\n",
        "        # For each band\n",
        "        for j in range(n_bands):\n",
        "            \n",
        "            # Perform linear interpolation and extrapolation\n",
        "            # nan timesteps are replaced by interpolated values\n",
        "            X[i, :, j] = np.interp(np.arange(n_timesteps), valid_timesteps, valid_values[:, j])\n",
        "    \n",
        "    # We only keep one timestep every sampling_rate\n",
        "    # Considering that Sentinel-2 revisit time is 5 days\n",
        "    # Sampling rate is 5 by default as there will be no additional information with a higher sampling rate\n",
        "    X = X[:, ::sampling_rate, :]\n",
        "\n",
        "    return X\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This results in a 3D numpy array of shape (n_objects, n_timesteps, n_bands) that can be flattened to a 2D numpy array of shape (n_objects, n_timesteps * n_bands) and fed to any supervised model.\n",
        "\n",
        "Below we show the first 50 NDVI training timeseries for each country :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 1, figsize=(15, 15))\n",
        "\n",
        "for country, ax in zip(ds.countries, axs):\n",
        "    country_mask = filter_by_country(ds._df, country)\n",
        "    ids = ds._df.loc[ds._df[IS_TRAIN] & country_mask, ID]\n",
        "    optical_filtered = ds._optical_df.loc[ds._optical_df[ID].isin(ids)]\n",
        "    X, IDs_ = interpolate_ts(optical_filtered, [NDVI])\n",
        "    Y = ds._df.loc[ds._df[ID].isin(ids)].set_index(ID).loc[IDs_][TARGET].values\n",
        "\n",
        "    X = X[:50]\n",
        "    Y = Y[:50]\n",
        "\n",
        "    ax.plot(X[Y == 0, :, 0].T, color='red', alpha=1)\n",
        "    ax.plot(X[Y == 1, :, 0].T, color='green', alpha=0.5)\n",
        "\n",
        "    # legend\n",
        "    ax.plot([], [], color='red', label='No cropland')\n",
        "    ax.plot([], [], color='green', label='Cropland')\n",
        "\n",
        "    ax.set_title(f'NDVI timeseries for {country.name}')\n",
        "    ax.set_xlabel('Timestep')\n",
        "    ax.set_ylabel('NDVI')\n",
        "    ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that, despite being noisy, timeseries of class 1 in green (cropland) have different NDVI profiles than timeseries of class 0 in red (non-cropland).  \n",
        "Being able to see visual difference between the two classes with a simple NDVI profile is a good sign.\n",
        "\n",
        "We can see many spikes in the timeseries, these are mostly due to clouds and cloud shadows.  \n",
        "Further preprocessing to obtain a cleaner timeseries might be necessary for some models but is not done in this notebook as we use a Random Forest which is robust to unclean data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Study on the impact of diffferent timerange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we study the impact of the timeseries timerange and timeperiod on the model performance.\n",
        "We train a RandomForest with the same parameters on different timeranges and timeperiods and compare the results.\n",
        "\n",
        "More precisely, we investigate timeranges of 1 month, 3 months, 6 months, 1 year, and 2 years.  \n",
        "We investigate timeperiods every 2 month strarting from 1 year before the given range to 1 year after the given range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "spans = ['4W', '12W', '24W', '48W', '96W']\n",
        "spans_friendly_unit = ['1M', '3M', '6M', '1Y', '2Y']\n",
        "freq = '2M'\n",
        "bands = [B2, B3, B4, B8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !!!!! This cell is relatively longer than others to run !!!!!\n",
        "# You can skip it and the next one if you d'ont want to recompute the results\n",
        "# ~ on Google Colab\n",
        "# ~3min on AMD 5900x CPU\n",
        "# Experiment\n",
        "countries_accs = []\n",
        "end_dates_in_experiment_countries = []\n",
        "for country in country_settings.values():\n",
        "    print(country[COUNTRY_NAME])\n",
        "\n",
        "    true_start_date = pd.to_datetime(country[START_DATE]) - pd.Timedelta(days=365)\n",
        "    true_end_date = pd.to_datetime(country[END_DATE]) + pd.Timedelta(days=365)\n",
        "\n",
        "    end_dates = pd.date_range(\n",
        "        start=true_start_date,\n",
        "        end=true_end_date,\n",
        "        freq=freq\n",
        "    )\n",
        "\n",
        "    accs_spans = []\n",
        "    end_dates_in_experiment_spans = []\n",
        "    for span, friendly_span in zip(spans, spans_friendly_unit):\n",
        "        accs_end_dates = []\n",
        "        end_dates_in_experiment = []\n",
        "        # print(f'    Span : {span}')\n",
        "        print(f'    Getting results for {friendly_span} timespan...')\n",
        "        for end_date in end_dates:\n",
        "            start_date = end_date - pd.Timedelta(span)\n",
        "            if start_date < true_start_date:\n",
        "                continue\n",
        "            \n",
        "            start_date = start_date.strftime('%Y-%m-%d')\n",
        "            end_date_ = end_date.strftime('%Y-%m-%d')\n",
        "            # print(f'        {start_date} - {end_date_} : ', end='')\n",
        "\n",
        "            ds_ts = Dataset_training_ready.get_ts_data_from(\n",
        "                ds, bands, PROJECT_NAME, country[COUNTRY_NAME], start_date, end_date_)\n",
        "\n",
        "            if ds_ts._df_optical.shape[0] == 0:\n",
        "                # print('No data found.')\n",
        "                continue\n",
        "            \n",
        "            model = Model(rf_builder_shallow, ds_ts, SEED)\n",
        "            acc, acc_0, acc_1, _, _ = model.train_with_cv_one_rf(debug_level=0, n_splits=2)\n",
        "            # print(f'{acc:.2f}')\n",
        "            accs_end_dates.append(acc)\n",
        "            end_dates_in_experiment.append(end_date)\n",
        "        accs_spans.append(accs_end_dates)\n",
        "        end_dates_in_experiment_spans.append(end_dates_in_experiment)\n",
        "    countries_accs.append(accs_spans)\n",
        "    end_dates_in_experiment_countries.append(end_dates_in_experiment_spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "optimal_parameters = [\n",
        "    ['2021-03-31', 0.965],\n",
        "    ['2023-04-30', 0.865],\n",
        "    ['2020-06-30', 0.955]\n",
        "]\n",
        "\n",
        "for country, accs_spans, end_dates_in_experiment_spans, optimal_parameter in zip(country_settings.values(), countries_accs, end_dates_in_experiment_countries, optimal_parameters):\n",
        "    fig, ax = plt.subplots(figsize=(14, 4))\n",
        "\n",
        "    for span, accs_end_dates, end_dates in zip(spans_friendly_unit, accs_spans, end_dates_in_experiment_spans):\n",
        "        ax.plot(end_dates, accs_end_dates, label=span)\n",
        "    \n",
        "    optimal_date = pd.to_datetime(optimal_parameter[0]).normalize()\n",
        "    optimal_acc = optimal_parameter[1]\n",
        "    ax.scatter(optimal_date, optimal_acc, color='red', label='Optimal')\n",
        "\n",
        "    true_start_date = pd.to_datetime(country[START_DATE]) - pd.Timedelta(days=365)\n",
        "    true_end_date = pd.to_datetime(country[END_DATE]) + pd.Timedelta(days=365)\n",
        "    end_dates = pd.date_range(\n",
        "        start=true_start_date,\n",
        "        end=true_end_date,\n",
        "        freq='2M'\n",
        "    )\n",
        "\n",
        "    plt.title(f'Accuracy on 2-fold CV for {country[COUNTRY_NAME]}')\n",
        "    plt.xlabel('End date of the timespan')\n",
        "    plt.ylabel('Accuracy on 2-fold CV')\n",
        "    plt.xticks(end_dates, rotation=45)\n",
        "    plt.xlim(true_start_date, true_end_date)\n",
        "    plt.yticks(list(np.arange(0.7, 1.01, 0.05)) + [0.86, 0.87, 0.96, 0.97, 0.98])\n",
        "    plt.ylim(0.67, 1)\n",
        "    \n",
        "    ax.axvline(pd.to_datetime(country[END_DATE]), color='blue', linestyle='dashed', label='Future limit')\n",
        "    ax.legend()\n",
        "    \n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this experiments, we can see that the best results are obtained with longer timeranges and saturates at around 1 year.  \n",
        "We can also see that Afghanistan has significantly worse results than the other countries with a max cross-validation accuracy of ~0.87 compared to ~0.97 for the other countries.\n",
        "\n",
        "The vertical blue dashed-line represents the time after which future data is required.  \n",
        "We can see that Afghanistan seems to benefit from future data while other countries don't.\n",
        "\n",
        "Finally, we can choose optimal timeranges and timeperiods for each country, they are represented by the red dots on the graph.  \n",
        "We choose them visually based on accuracy and stability around the optimal timeperiod.\n",
        "\n",
        "Please see Section 6.3 for additional results showing performance using only past data.\n",
        "\n",
        "| Country | Timerange | Start | End | Acc |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| Sudan | 2 years | 2019-05-29 | 2021-03-29 | 0.97 |\n",
        "| Afghanistan | 2 years | 2021-06-27 | 2023-04-30 | 0.87 |\n",
        "| Iran | 2 years | 2018-08-28 | 2020-06-30 | 0.96 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "country_settings_optimal = {\n",
        "    SUDAN: {\n",
        "        COUNTRY_NAME: SUDAN,\n",
        "        START_DATE: '2019-05-29',\n",
        "        END_DATE: '2021-03-31',\n",
        "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
        "    },\n",
        "    AFGHANISTAN: {\n",
        "        COUNTRY_NAME: AFGHANISTAN,\n",
        "        START_DATE: '2021-06-27',\n",
        "        END_DATE: '2023-04-30',\n",
        "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
        "    },\n",
        "    IRAN: {\n",
        "        COUNTRY_NAME: IRAN,\n",
        "        START_DATE: '2018-08-28',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Study on the impact of the choice of Sentinel-2 bands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we investigate which Sentinel-2 bands are the most useful.  \n",
        "We train a RandomForest with the same parameters on different band combinations and compare the results.  \n",
        "Using all bands is not necessarily the best option as this will increase training time and memory usage and might lead to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !!!!! This cell is relatively longer than others to run !!!!!\n",
        "# It can be skipped if you don't want to recompute the results\n",
        "# ~ on Google Colab\n",
        "# ~1min on AMD 5900x CPU\n",
        "bandssets = [\n",
        "    [SCL],\n",
        "    [B3, B8],\n",
        "    [B3, B4, B8],\n",
        "    [B2, B3, B4, B8],\n",
        "    [NDVI], \n",
        "    [B2, B3, B4, B8, NDVI, SCL],\n",
        "    [B2, B3, B4, B8, LON, LAT, NDVI, SCL],\n",
        "    ALL_BANDS\n",
        "]\n",
        "\n",
        "for bands in bandssets:\n",
        "    ds_ts = Dataset_training_ready.get_ts_data_from(\n",
        "        ds, bands.copy(), PROJECT_NAME, country_settings=country_settings_optimal)\n",
        "    model = Model(rf_builder_shallow, ds_ts, SEED)\n",
        "    acc, acc_0, acc_1, _, _ = model.train_with_cv_one_rf_per_country(debug_level=0, n_splits=5)\n",
        "    print(f'   Accuracy : {acc:.3f} with {bands}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that even with only the NDVI band, we can achieve a high cross-validation accuracy of ~0.93.  \n",
        "For the final submission, we choose [B2, B3, B4, B8, Lon, Lat, NDVI, SCL].  \n",
        "We've decided to keep Lon, Lat and SCL despite no significant accuracy improvment as they provide information different in nature than radiometric bands and might be useful to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Reproduction of the submitted solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### PLEASE SET THIS VARIABLE TO YOUR PROJECT NAME IN GEE #####\n",
        "PROJECT_NAME = \"ee-antoinesaget\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and seeds initializations\n",
        "import random\n",
        "import ee\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from src.dataset import Dataset, Dataset_training_ready\n",
        "from src.model import Model, rf_builder_shallow\n",
        "from src.constants import SUDAN, AFGHANISTAN, IRAN, COUNTRY_NAME, START_DATE, END_DATE, BOUNDS, TARGET, B2, B3, B4, B8, LON, LAT, NDVI, SCL\n",
        "from src.utils import save_submission\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 2023\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skip this cell if you already authenticated and initialized EE before\n",
        "\n",
        "# Authenticate and initialize Earth Engine\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Country bounds and timeranges\n",
        "country_settings = {\n",
        "   SUDAN: {\n",
        "        COUNTRY_NAME: SUDAN,\n",
        "        START_DATE: '2019-07-01',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
        "    },\n",
        "    AFGHANISTAN: {\n",
        "        COUNTRY_NAME: AFGHANISTAN,\n",
        "        START_DATE: '2022-04-01',\n",
        "        END_DATE: '2022-04-30',\n",
        "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
        "    },\n",
        "    IRAN: {\n",
        "        COUNTRY_NAME: IRAN,\n",
        "        START_DATE: '2019-07-01',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = Dataset.from_files('Train.csv', 'Test.csv', 'Full dataset', country_settings, debug_level=0)\n",
        "# With pandas==2.1.1 there might be a warning about the column type, it can be ignored\n",
        "# as it is fixed in next versions of pandas : https://github.com/pandas-dev/pandas/issues/55025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Please see 4. for more informations on bands choices\n",
        "bands = [B2, B3, B4, B8, LON, LAT, NDVI, SCL]\n",
        "\n",
        "# Please see 3. for more informations on timespans choices\n",
        "country_settings_optimal = {\n",
        "    SUDAN: {\n",
        "        COUNTRY_NAME: SUDAN,\n",
        "        START_DATE: '2019-05-29',\n",
        "        END_DATE: '2021-03-31',\n",
        "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
        "    },\n",
        "    AFGHANISTAN: {\n",
        "        COUNTRY_NAME: AFGHANISTAN,\n",
        "        START_DATE: '2021-06-27',\n",
        "        END_DATE: '2023-04-30',\n",
        "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
        "    },\n",
        "    IRAN: {\n",
        "        COUNTRY_NAME: IRAN,\n",
        "        START_DATE: '2018-08-28',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Loading optical data\n",
        "ds_ts = Dataset_training_ready.get_ts_data_from(\n",
        "    ds, bands, PROJECT_NAME, country_settings=country_settings_optimal)\n",
        "\n",
        "# Training and predicting\n",
        "# Our final model is a single shallow random forest (per country) of 100 trees and a max depth of 10.\n",
        "model = Model(rf_builder_shallow, ds_ts, SEED)\n",
        "model.train_on_full_dataset_one_per_country()\n",
        "preds, ids = model.predict_on_test()\n",
        "\n",
        "# Check diff between original submission and current predisions\n",
        "original = pd.read_csv('submissions/original_challenge_submission.csv', index_col='ID', usecols=['ID', TARGET])\n",
        "original[TARGET] = original[TARGET].astype('uint8')\n",
        "\n",
        "diff = original.loc[ids, TARGET] - preds\n",
        "diff = diff[diff != 0] # True on rows different from original submission, False otherwise\n",
        "print(f'Number of predictions different from original submission : {len(diff)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_submission(preds, ids, 'reproduction_of_original_submission')\n",
        "# Please note that a diff with the original submission and this one will not be 0.\n",
        "# By mistake, the original submission also included predictions of the training set.\n",
        "# This mean that the original submission is 3000 rows while this one is 1500 rows (only test set)\n",
        "# But the predictions on the test set are the same (as shown in the above cell)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Discussion on strengths, weaknesses and possible improvements\n",
        "\n",
        "In this notebook we've shown that a simple Random Forest model paired with Sentinel-2 timeseries can achieve a high accuracy on the GEO-AI Challenge for Cropland Mapping by ITU challenge.\n",
        "\n",
        "The strenghts of this solution are:\n",
        "- Simple model that is easy to train\n",
        "- Can scale to larger datasets\n",
        "- Can be trained easily on CPU in a few minutes on Google Colab\n",
        "- Can be trained on GPU if dataset is very large (with NVIDIA Rapids cuML RandomForest implementation for exemple)\n",
        "- Can be adjusted to use less memory by reducing the number of timesteps or bands without significant accuracy loss (see 6.1)\n",
        "- Is robust to unclean data (data is almost used raw from GEE)\n",
        "\n",
        "The weaknesses of this solution are:\n",
        "- The simple preprocessing might not be enough for other models, especially models sensitive to outliers or scale. For example standardization or normalization might be necessary for Neural Networks.\n",
        "- The GEE download is long. Optimizing the GEE pipeline might be necessary for larger datasets if time is an issue.\n",
        "- The data interpolation and reindexing is long when compared to the training time of the model. Further optimization might be necessary for larger datasets if time is an issue.\n",
        "- Using timeseries instead of mean over a given period (as shown in the baseline notebook) increase the training time and memory usage. However, this is not an issue with the current dataset size.\n",
        "- Currenlty the best results are obtained using future data. If the model is to be used in real-time, only past data can be used. This can lead to a slight decrease in accuracy as shown in 6.2.\n",
        "- Currently the model is weaker on Afghanistan than on other countries. Further investigation might be necessary to understand why.\n",
        "- Currently, the Lon and Lat columns are repeated for each timestep. This is not optimal.\n",
        "\n",
        "Possible improvements:\n",
        "- Ensembling multiple RFs with different parameters (such as a wide, a shallow and a normal RF) might improve the results.\n",
        "- Adding Land cover classification from the Dynamic World V1 collection as an additional feature might improve the results. This collection is already the result of a classification but from a model trained with 5 billion pixels of training data. \n",
        "- As shown in https://arxiv.org/abs/1905.11893 transformer models outperform RFs on Sentinel-2 timeseries. This might be a good improvement to try over our current model provided that preprocessing is done correctly.\n",
        "- Also, considering the amount of Sentinel-2 unlabeled data available, a semi-supervised approach might be a good improvement to try. Self-supervised pretraining of a large quantity of unlabeled data followed by finetuning on the labeled data for example. However, this will be more difficult to implement with longer training time and more memory usage (probably requiring a GPU).\n",
        "\n",
        "In the remainder of this section, we show results obtained on the private leaderboard with different models and data to help the reader choose the best model for his use case.\n",
        "These results go beyond the scope of only reproducing the submitted solution but might be useful to the reader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 NDVI vs. Challenge submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bands = [NDVI]\n",
        "country_settings_optimal = {\n",
        "    SUDAN: {\n",
        "        COUNTRY_NAME: SUDAN,\n",
        "        START_DATE: '2019-05-29',\n",
        "        END_DATE: '2021-03-31',\n",
        "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
        "    },\n",
        "    AFGHANISTAN: {\n",
        "        COUNTRY_NAME: AFGHANISTAN,\n",
        "        START_DATE: '2021-06-27',\n",
        "        END_DATE: '2023-04-30',\n",
        "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
        "    },\n",
        "    IRAN: {\n",
        "        COUNTRY_NAME: IRAN,\n",
        "        START_DATE: '2018-08-28',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Loading optical data\n",
        "ds_ts = Dataset_training_ready.get_ts_data_from(\n",
        "    ds, bands, PROJECT_NAME, country_settings=country_settings_optimal)\n",
        "ds_ts.X_train.info()\n",
        "# Training and predicting\n",
        "# Our final model is a single shallow random forest (per country) of 100 trees and a max depth of 10.\n",
        "model = Model(rf_builder_shallow, ds_ts, SEED)\n",
        "start = time.time()\n",
        "model.train_on_full_dataset_one_per_country()\n",
        "preds, ids = model.predict_on_test()\n",
        "end = time.time()\n",
        "print(f'Training + Prediction time : {end - start:.2f}s')\n",
        "save_submission(preds, ids, 'ndvi_only')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Bands | Acc on private leaderboard | Train + test time with AMD Ryzen 9 5900x | Train + test time on Google Colab | X_train size | \n",
        "| --- | --- | --- | --- | --- |\n",
        "| B2, B3, B4, B8, LON, LAT, NDVI, SCL (challenge submission) | 0.943 | 5.1s |  | 31.1 MB|\n",
        "| NDVI | 0.938 | 3.2s | | 1.8 MB |\n",
        "\n",
        "Using only the NDVI band, we can still achieve a high accuracy of 0.938 on the private leaderboard.\n",
        "The training and test time is not significantly reduced as the data preprocessing is the bottleneck.\n",
        "The memory usage is significantly reduced from 31.1 MB to 1.8 MB when using only the NDVI band."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6.2 Future vs. Only past data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bands = [B2, B3, B4, B8, LON, LAT, NDVI, SCL]\n",
        "country_settings_optimal = {\n",
        "    SUDAN: {\n",
        "        COUNTRY_NAME: SUDAN,\n",
        "        START_DATE: '2019-01-01',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
        "    },\n",
        "    AFGHANISTAN: {\n",
        "        COUNTRY_NAME: AFGHANISTAN,\n",
        "        START_DATE: '2021-04-01',\n",
        "        END_DATE: '2022-04-30',\n",
        "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
        "    },\n",
        "    IRAN: {\n",
        "        COUNTRY_NAME: IRAN,\n",
        "        START_DATE: '2018-08-28',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Loading optical data\n",
        "ds_ts = Dataset_training_ready.get_ts_data_from(\n",
        "    ds, bands, PROJECT_NAME, country_settings=country_settings_optimal)\n",
        "\n",
        "# Training and predicting\n",
        "# Our final model is a single shallow random forest (per country) of 100 trees and a max depth of 10.\n",
        "model = Model(rf_builder_shallow, ds_ts, SEED)\n",
        "model.train_on_full_dataset_one_per_country()\n",
        "preds, ids = model.predict_on_test()\n",
        "\n",
        "save_submission(preds, ids, 'only_past')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Data | Acc on private leaderboard | \n",
        "| --- | --- |\n",
        "| Future and past data (challenge submission) | 0.943 |\n",
        "| Past data only | 0.932 |\n",
        "\n",
        "Cutting the optimal timerange to stop at the present instead of the future leads to a slight decrease in accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6.3 One model per country vs. One model for all countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bands = [B2, B3, B4, B8, LON, LAT, NDVI, SCL]\n",
        "country_settings_optimal = {\n",
        "    SUDAN: {\n",
        "        COUNTRY_NAME: SUDAN,\n",
        "        START_DATE: '2019-05-29',\n",
        "        END_DATE: '2021-03-31',\n",
        "        BOUNDS: [[14.1, 33.1], [14.6, 33.6]]\n",
        "    },\n",
        "    AFGHANISTAN: {\n",
        "        COUNTRY_NAME: AFGHANISTAN,\n",
        "        START_DATE: '2021-06-27',\n",
        "        END_DATE: '2023-04-30',\n",
        "        BOUNDS: [[34.0, 70.2], [34.4, 70.8]]\n",
        "    },\n",
        "    IRAN: {\n",
        "        COUNTRY_NAME: IRAN,\n",
        "        START_DATE: '2018-08-28',\n",
        "        END_DATE: '2020-06-30',\n",
        "        BOUNDS: [[32.0, 48.1], [32.5, 48.6]]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Loading optical data\n",
        "ds_ts = Dataset_training_ready.get_ts_data_from(\n",
        "    ds, bands, PROJECT_NAME, country_settings=country_settings_optimal)\n",
        "# Training and predicting\n",
        "# Our final model is a single shallow random forest (per country) of 100 trees and a max depth of 10.\n",
        "model = Model(rf_builder_shallow, ds_ts, SEED)\n",
        "model.train_on_full_dataset()\n",
        "preds, ids = model.predict_on_test()\n",
        "save_submission(preds, ids, 'one_single_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Model | Acc on private leaderboard |\n",
        "| --- | --- |\n",
        "| One model per country (challenge submission) | 0.943 |\n",
        "| One model for all countries | 0.934 |\n",
        "\n",
        "Training only one model instead of one model per country leads to a slight decrease in accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
